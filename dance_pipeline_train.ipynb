{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c97845-d18f-4be3-bac0-0acb6f9a7f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from gdown) (3.14.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from gdown) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from gdown) (4.65.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/monone/miniconda3/envs/openmmlab/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428143e-7292-4174-954e-74936411925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=18lLfz1uFNoOwf_nzLvMNgEbSF4C29zaz\n",
      "From (redirected): https://drive.google.com/uc?id=18lLfz1uFNoOwf_nzLvMNgEbSF4C29zaz&confirm=t&uuid=8636eb27-f9bb-4669-9704-764c9bb2cb43\n",
      "To: /Users/monone/mmaction2/data/k700-2020.zip\n",
      "  7%|███████████                                                                                                                                          | 1.47G/19.8G [04:39<13:18, 23.0MB/s]"
     ]
    }
   ],
   "source": [
    "# Step 0: Download the dataset from Google Drive if not already downloaded\n",
    "import os\n",
    "import gdown # from command line: $ \n",
    "import zipfile\n",
    "\n",
    "# Function to download dataset from Google Drive\n",
    "def download_dataset(drive_url, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        gdown.download(drive_url, output_path, quiet=False)\n",
    "    else:\n",
    "        print(f\"{output_path} already exists. Skipping download.\")\n",
    "\n",
    "# Function to unzip the dataset\n",
    "def unzip_dataset(zip_path, extract_to):\n",
    "    if not os.path.exists(extract_to):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "    else:\n",
    "        print(f\"{extract_to} already exists. Skipping extraction.\")\n",
    "\n",
    "# Google Drive URL and output paths\n",
    "drive_url = \"https://drive.google.com/uc?id=18lLfz1uFNoOwf_nzLvMNgEbSF4C29zaz\"\n",
    "output_zip_path = \"data/k700-2020.zip\"\n",
    "extract_to_path = \"data/k700-2020\"\n",
    "\n",
    "# Download and unzip dataset\n",
    "download_dataset(drive_url, output_zip_path)\n",
    "unzip_dataset(output_zip_path, extract_to_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd2b8a-5cd4-49b7-a181-7cd300792f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.utils import register_all_modules\n",
    "\n",
    "register_all_modules(init_default_scope=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0324b7-af2f-4110-a918-6ca1539fa428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "import decord\n",
    "import numpy as np\n",
    "from mmcv.transforms import TRANSFORMS, BaseTransform, to_tensor\n",
    "from mmaction.structures import ActionDataSample\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoInit(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        container = decord.VideoReader(results['filename'])\n",
    "        results['total_frames'] = len(container)\n",
    "        results['video_reader'] = container\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoSample(BaseTransform):\n",
    "    def __init__(self, clip_len, num_clips, test_mode=False):\n",
    "        self.desired_clip_len = clip_len\n",
    "        self.num_clips = num_clips\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def transform(self, results):\n",
    "        total_frames = results['total_frames']\n",
    "        self.clip_len = min(self.desired_clip_len, total_frames)  # Adjust clip length dynamically\n",
    "        interval = total_frames // self.clip_len\n",
    "\n",
    "        if interval <= 0:\n",
    "            print(f\"Invalid interval detected! Filename: {results['filename']}, Total frames: {total_frames}, Clip length: {self.clip_len}, Interval: {interval}\")\n",
    "            interval = 1  # Set a minimum interval value to avoid errors\n",
    "\n",
    "        if self.test_mode:\n",
    "            # Make the sampling during testing deterministic\n",
    "            np.random.seed(42)\n",
    "\n",
    "        inds_of_all_clips = []\n",
    "        for i in range(self.num_clips):\n",
    "            bids = np.arange(self.clip_len) * interval\n",
    "            offset = np.random.randint(interval, size=bids.shape)\n",
    "            inds = bids + offset\n",
    "            inds_of_all_clips.append(inds)\n",
    "\n",
    "        results['frame_inds'] = np.concatenate(inds_of_all_clips)\n",
    "        results['clip_len'] = self.clip_len\n",
    "        results['num_clips'] = self.num_clips\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoDecode(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        frame_inds = results['frame_inds']\n",
    "        container = results['video_reader']\n",
    "\n",
    "        imgs = container.get_batch(frame_inds).asnumpy()\n",
    "        imgs = list(imgs)\n",
    "\n",
    "        results['video_reader'] = None\n",
    "        del container\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoResize(BaseTransform):\n",
    "    def __init__(self, r_size):\n",
    "        self.r_size = (np.inf, r_size)\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        new_w, new_h = mmcv.rescale_size((img_w, img_h), self.r_size)\n",
    "\n",
    "        imgs = [mmcv.imresize(img, (new_w, new_h))\n",
    "                for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoCrop(BaseTransform):\n",
    "    def __init__(self, c_size):\n",
    "        self.c_size = c_size\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        center_x, center_y = img_w // 2, img_h // 2\n",
    "        x1, x2 = center_x - self.c_size // 2, center_x + self.c_size // 2\n",
    "        y1, y2 = center_y - self.c_size // 2, center_y + self.c_size // 2\n",
    "        imgs = [img[y1:y2, x1:x2] for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoFormat(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        num_clips = results['num_clips']\n",
    "        clip_len = results['clip_len']\n",
    "        imgs = results['imgs']\n",
    "\n",
    "        # [num_clips*clip_len, H, W, C]\n",
    "        imgs = np.array(imgs)\n",
    "        # [num_clips, clip_len, H, W, C]\n",
    "        imgs = imgs.reshape((num_clips, clip_len) + imgs.shape[1:])\n",
    "        # [num_clips, C, clip_len, H, W]\n",
    "        imgs = imgs.transpose(0, 4, 1, 2, 3)\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoPack(BaseTransform):\n",
    "    def __init__(self, meta_keys=('img_shape', 'num_clips', 'clip_len')):\n",
    "        self.meta_keys = meta_keys\n",
    "\n",
    "    def transform(self, results):\n",
    "        packed_results = dict()\n",
    "        inputs = to_tensor(results['imgs'])\n",
    "        data_sample = ActionDataSample()\n",
    "        data_sample.set_gt_label(results['label'])\n",
    "        metainfo = {k: results[k] for k in self.meta_keys if k in results}\n",
    "        data_sample.set_metainfo(metainfo)\n",
    "        packed_results['inputs'] = inputs\n",
    "        packed_results['data_samples'] = data_sample\n",
    "        return packed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49acf44-1975-4901-b8a0-eb69cbefedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "pipeline = Compose(pipeline_cfg)\n",
    "data_prefix = 'data/k700-2020/updated_splits/train'\n",
    "results = dict(filename=osp.join(data_prefix, 'vmwkhajXQP4_000004_000014.mp4'), label=0)\n",
    "packed_results = pipeline(results)\n",
    "\n",
    "inputs = packed_results['inputs']\n",
    "data_sample = packed_results['data_samples']\n",
    "\n",
    "print('shape of the inputs: ', inputs.shape)\n",
    "\n",
    "# Get metainfo of the inputs\n",
    "print('image_shape: ', data_sample.img_shape)\n",
    "print('num_clips: ', data_sample.num_clips)\n",
    "print('clip_len: ', data_sample.clip_len)\n",
    "\n",
    "# Get label of the inputs\n",
    "print('label: ', data_sample.gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde34a4-c5b2-4230-8eb0-e93a49700dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from mmengine.fileio import list_from_file\n",
    "from mmengine.dataset import BaseDataset\n",
    "from mmaction.registry import DATASETS\n",
    "\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class DatasetZelda(BaseDataset):\n",
    "    def __init__(self, ann_file, pipeline, data_root, data_prefix=dict(video=''),\n",
    "                 test_mode=False, modality='RGB', **kwargs):\n",
    "        self.modality = modality\n",
    "        super(DatasetZelda, self).__init__(ann_file=ann_file, pipeline=pipeline, data_root=data_root,\n",
    "                                           data_prefix=data_prefix, test_mode=test_mode,\n",
    "                                           **kwargs)\n",
    "\n",
    "    def load_data_list(self):\n",
    "        data_list = []\n",
    "        fin = list_from_file(self.ann_file)\n",
    "        for line in fin:\n",
    "            line_split = line.strip().split()\n",
    "            filename, label = line_split\n",
    "            label = int(label)\n",
    "            filename = osp.join(self.data_prefix['video'], filename)\n",
    "            data_list.append(dict(filename=filename, label=label))\n",
    "        return data_list\n",
    "\n",
    "    def get_data_info(self, idx: int) -> dict:\n",
    "        data_info = super().get_data_info(idx)\n",
    "        data_info['modality'] = self.modality\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff558c-fe74-4088-bd96-20f704d07990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.registry import DATASETS\n",
    "\n",
    "train_pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "val_pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=5, test_mode=True),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "train_dataset_cfg = dict(\n",
    "    type='DatasetZelda',\n",
    "    ann_file='train.txt',\n",
    "    pipeline=train_pipeline_cfg,\n",
    "    data_root='data/k700-2020/updated_splits/',\n",
    "    data_prefix=dict(video='train'))\n",
    "\n",
    "val_dataset_cfg = dict(\n",
    "    type='DatasetZelda',\n",
    "    ann_file='val.txt',\n",
    "    pipeline=val_pipeline_cfg,\n",
    "    data_root='data/k700-2020/updated_splits/',\n",
    "    data_prefix=dict(video='val'))\n",
    "\n",
    "train_dataset = DATASETS.build(train_dataset_cfg)\n",
    "\n",
    "packed_results = train_dataset[0]\n",
    "\n",
    "inputs = packed_results['inputs']\n",
    "data_sample = packed_results['data_samples']\n",
    "\n",
    "print('shape of the inputs: ', inputs.shape)\n",
    "\n",
    "# Get metainfo of the inputs\n",
    "print('image_shape: ', data_sample.img_shape)\n",
    "print('num_clips: ', data_sample.num_clips)\n",
    "print('clip_len: ', data_sample.clip_len)\n",
    "\n",
    "# Get label of the inputs\n",
    "print('label: ', data_sample.gt_label)\n",
    "\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataloader_cfg = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    dataset=train_dataset_cfg)\n",
    "\n",
    "val_dataloader_cfg = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=val_dataset_cfg)\n",
    "\n",
    "train_data_loader = Runner.build_dataloader(dataloader=train_dataloader_cfg)\n",
    "val_data_loader = Runner.build_dataloader(dataloader=val_dataloader_cfg)\n",
    "\n",
    "batched_packed_results = next(iter(train_data_loader))\n",
    "\n",
    "batched_inputs = batched_packed_results['inputs']\n",
    "batched_data_sample = batched_packed_results['data_samples']\n",
    "\n",
    "assert len(batched_inputs) == BATCH_SIZE\n",
    "assert len(batched_data_sample) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f575ae-b72b-41a0-a8b6-773aeea334f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mmengine.model import BaseDataPreprocessor, stack_batch\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class DataPreprocessorZelda(BaseDataPreprocessor):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mean',\n",
    "            torch.tensor(mean, dtype=torch.float32).view(-1, 1, 1, 1),\n",
    "            False)\n",
    "        self.register_buffer(\n",
    "            'std',\n",
    "            torch.tensor(std, dtype=torch.float32).view(-1, 1, 1, 1),\n",
    "            False)\n",
    "\n",
    "    def forward(self, data, training=False):\n",
    "        data = self.cast_data(data)\n",
    "        inputs = data['inputs']\n",
    "        batch_inputs = stack_batch(inputs)  # Batching\n",
    "        batch_inputs = (batch_inputs - self.mean) / self.std  # Normalization\n",
    "        data['inputs'] = batch_inputs\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f7074-3ec5-4483-8aa3-07038925529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.registry import MODELS\n",
    "\n",
    "data_preprocessor_cfg = dict(\n",
    "    type='DataPreprocessorZelda',\n",
    "    mean=[123.675, 116.28, 103.53],\n",
    "    std=[58.395, 57.12, 57.375])\n",
    "\n",
    "data_preprocessor = MODELS.build(data_preprocessor_cfg)\n",
    "\n",
    "preprocessed_inputs = data_preprocessor(batched_packed_results)\n",
    "print(preprocessed_inputs['inputs'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c303d8-596f-4798-a13a-cc7fe7fc8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmengine.model import BaseModel, BaseModule, Sequential\n",
    "from mmengine.structures import LabelData\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class BackBoneZelda(BaseModule):\n",
    "    def __init__(self, init_cfg=None):\n",
    "        if init_cfg is None:\n",
    "            init_cfg = [dict(type='Kaiming', layer='Conv3d', mode='fan_out', nonlinearity=\"relu\"),\n",
    "                        dict(type='Constant', layer='BatchNorm3d', val=1, bias=0)]\n",
    "\n",
    "        super(BackBoneZelda, self).__init__(init_cfg=init_cfg)\n",
    "\n",
    "        self.conv1 = Sequential(nn.Conv3d(3, 64, kernel_size=(3, 7, 7),\n",
    "                                          stride=(1, 2, 2), padding=(1, 3, 3)),\n",
    "                                nn.BatchNorm3d(64), nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2),\n",
    "                                    padding=(0, 1, 1))\n",
    "\n",
    "        self.conv = Sequential(nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                               nn.BatchNorm3d(128), nn.ReLU())\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # imgs: [batch_size*num_views, 3, T, H, W]\n",
    "        # features: [batch_size*num_views, 128, T/2, H//8, W//8]\n",
    "        features = self.conv(self.maxpool(self.conv1(imgs)))\n",
    "        return features\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class ClsHeadZelda(BaseModule):\n",
    "    def __init__(self, num_classes, in_channels, dropout=0.5, average_clips='prob', init_cfg=None):\n",
    "        if init_cfg is None:\n",
    "            init_cfg = dict(type='Normal', layer='Linear', std=0.01)\n",
    "\n",
    "        super(ClsHeadZelda, self).__init__(init_cfg=init_cfg)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.average_clips = average_clips\n",
    "\n",
    "        if dropout != 0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        self.fc = nn.Linear(self.in_channels, self.num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, T, H, W = x.shape\n",
    "        x = self.pool(x)\n",
    "        x = x.view(N, C)\n",
    "        assert x.shape[1] == self.in_channels\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        cls_scores = self.fc(x)\n",
    "        return cls_scores\n",
    "\n",
    "    def loss(self, feats, data_samples):\n",
    "        cls_scores = self(feats)\n",
    "        labels = torch.stack([x.gt_label for x in data_samples])\n",
    "        labels = labels.squeeze()\n",
    "\n",
    "        if labels.shape == torch.Size([]):\n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "        loss_cls = self.loss_fn(cls_scores, labels)\n",
    "        return dict(loss_cls=loss_cls)\n",
    "\n",
    "    def predict(self, feats, data_samples):\n",
    "        cls_scores = self(feats)\n",
    "        num_views = cls_scores.shape[0] // len(data_samples)\n",
    "        # assert num_views == data_samples[0].num_clips\n",
    "        cls_scores = self.average_clip(cls_scores, num_views)\n",
    "\n",
    "        for ds, sc in zip(data_samples, cls_scores):\n",
    "            pred = LabelData(item=sc)\n",
    "            ds.pred_scores = pred\n",
    "        return data_samples\n",
    "\n",
    "    def average_clip(self, cls_scores, num_views):\n",
    "          if self.average_clips not in ['score', 'prob', None]:\n",
    "            raise ValueError(f'{self.average_clips} is not supported. '\n",
    "                             f'Currently supported ones are '\n",
    "                             f'[\"score\", \"prob\", None]')\n",
    "\n",
    "          total_views = cls_scores.shape[0]\n",
    "          cls_scores = cls_scores.view(total_views // num_views, num_views, -1)\n",
    "\n",
    "          if self.average_clips is None:\n",
    "              return cls_scores\n",
    "          elif self.average_clips == 'prob':\n",
    "              cls_scores = F.softmax(cls_scores, dim=2).mean(dim=1)\n",
    "          elif self.average_clips == 'score':\n",
    "              cls_scores = cls_scores.mean(dim=1)\n",
    "\n",
    "          return cls_scores\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class RecognizerZelda(BaseModel):\n",
    "    def __init__(self, backbone, cls_head, data_preprocessor):\n",
    "        super().__init__(data_preprocessor=data_preprocessor)\n",
    "\n",
    "        self.backbone = MODELS.build(backbone)\n",
    "        self.cls_head = MODELS.build(cls_head)\n",
    "\n",
    "    def extract_feat(self, inputs):\n",
    "        inputs = inputs.view((-1, ) + inputs.shape[2:])\n",
    "        return self.backbone(inputs)\n",
    "\n",
    "    def loss(self, inputs, data_samples):\n",
    "        feats = self.extract_feat(inputs)\n",
    "        loss = self.cls_head.loss(feats, data_samples)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, data_samples):\n",
    "        feats = self.extract_feat(inputs)\n",
    "        predictions = self.cls_head.predict(feats, data_samples)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, inputs, data_samples=None, mode='tensor'):\n",
    "        if mode == 'tensor':\n",
    "            return self.extract_feat(inputs)\n",
    "        elif mode == 'loss':\n",
    "            return self.loss(inputs, data_samples)\n",
    "        elif mode == 'predict':\n",
    "            return self.predict(inputs, data_samples)\n",
    "        else:\n",
    "            raise RuntimeError(f'Invalid mode: {mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5d8ed-2079-4fd9-8fca-262c1c537d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "model_cfg = dict(\n",
    "    type='RecognizerZelda',\n",
    "    backbone=dict(type='BackBoneZelda'),\n",
    "    cls_head=dict(\n",
    "        type='ClsHeadZelda',\n",
    "        num_classes=19,\n",
    "        in_channels=128,\n",
    "        average_clips='prob'),\n",
    "    data_preprocessor = dict(\n",
    "        type='DataPreprocessorZelda',\n",
    "        mean=[123.675, 116.28, 103.53],\n",
    "        std=[58.395, 57.12, 57.375]))\n",
    "\n",
    "model = MODELS.build(model_cfg)\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "model.init_weights()\n",
    "data_batch_train = copy.deepcopy(batched_packed_results)\n",
    "data = model.data_preprocessor(data_batch_train, training=True)\n",
    "loss = model(**data, mode='loss')\n",
    "print('loss dict: ', loss)\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    data_batch_test = copy.deepcopy(batched_packed_results)\n",
    "    data = model.data_preprocessor(data_batch_test, training=False)\n",
    "    predictions = model(**data, mode='predict')\n",
    "print('Label of Sample[0]', predictions[0].gt_label)\n",
    "print('Scores of Sample[0]', predictions[0].pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d2b1e-0a34-43d8-a004-41103d7c9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmaction.evaluation import top_k_accuracy\n",
    "from mmaction.registry import METRICS\n",
    "\n",
    "\n",
    "@METRICS.register_module()\n",
    "class AccuracyMetric(BaseMetric):\n",
    "    def __init__(self, topk=(1, 5), collect_device='cpu', prefix='acc'):\n",
    "        super().__init__(collect_device=collect_device, prefix=prefix)\n",
    "        self.topk = topk\n",
    "\n",
    "    def process(self, data_batch, data_samples):\n",
    "        data_samples = copy.deepcopy(data_samples)\n",
    "        for data_sample in data_samples:\n",
    "            result = dict()\n",
    "            #scores = data_sample['pred_score'].cpu().numpy()\n",
    "            #label = data_sample['gt_label'].item()\n",
    "            scores = data_sample['pred_scores']['item'].cpu().numpy()\n",
    "            label = data_sample['gt_label'].item()\n",
    "            result['scores'] = scores\n",
    "            result['label'] = label\n",
    "            self.results.append(result)\n",
    "\n",
    "    def compute_metrics(self, results: list) -> dict:\n",
    "        eval_results = OrderedDict()\n",
    "        labels = [res['label'] for res in results]\n",
    "        scores = [res['scores'] for res in results]\n",
    "        topk_acc = top_k_accuracy(scores, labels, self.topk)\n",
    "        for k, acc in zip(self.topk, topk_acc):\n",
    "            eval_results[f'topk{k}'] = acc\n",
    "        return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5186173-53a2-42fc-9eb9-399ecffa3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.registry import METRICS\n",
    "\n",
    "metric_cfg = dict(type='AccuracyMetric', topk=(1, 5))\n",
    "\n",
    "metric = METRICS.build(metric_cfg)\n",
    "\n",
    "data_samples = [d.to_dict() for d in predictions]\n",
    "\n",
    "metric.process(batched_packed_results, data_samples)\n",
    "acc = metric.compute_metrics(metric.results)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806d525-dad5-48d0-af9c-c8f6c85c265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'  # or 'cpu'\n",
    "max_epochs = 10\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for data_batch in tqdm(train_data_loader):\n",
    "        data = model.data_preprocessor(data_batch, training=True)\n",
    "        loss_dict = model(**data, mode='loss')\n",
    "        loss = loss_dict['loss_cls']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch[{epoch}]: loss ', sum(losses) / len(train_data_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_batch in tqdm(val_data_loader):\n",
    "            data = model.data_preprocessor(data_batch, training=False)\n",
    "            predictions = model(**data, mode='predict')\n",
    "            data_samples = [d.to_dict() for d in predictions]\n",
    "            metric.process(data_batch, data_samples)\n",
    "\n",
    "        acc = metric.compute_metrics(metric.results)\n",
    "        for name, topk in acc.items():\n",
    "            print(f'{name}: ', topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb299ddb-affb-4a41-95bc-27c7b47c584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8f1cc-276e-40f4-900c-7560bd5bc363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
